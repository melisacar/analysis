https://github.com/odenipinedo/Python/blob/master/datacamp/preprocessing%20for%20machine%20learning%20in%20Python.ipynb

What is data preprocessing?

beyond cleaning and exploratory data analysis
prepping data for modeling e.g. transforming categorical data to numeric
Pandas
.columns
.dtypes
.describe()
remove missing data
.dropna() - drop rows with NA values (axis=0, thresh=1)
df["B"].isnull().sum() - sum of all null values for specific column
df[df["B"].notnull()] - index for values that are not null for specific columns
.drop([1, 2, 3]) - drop specific rows
.drop("A", axis = 1) - drop specific columns
df[df["B"] == 7] - boolean indexing of columns

===========
# Check how many values are missing in the category_desc column
#print(volunteer["category_desc"].isnull().sum())

# Subset the volunteer dataset
#volunteer_subset = volunteer[volunteer["category_desc"].notnull()]

# Print out the shape of the subset
#print(volunteer_subset.shape)

#################################################
#<script.py> output:
#    48
#    (617, 35)
#################################################

------------------------------------------------------------------------------------------------------------
Working with data types

Why are types important?
.dtypes
object - string/mixed types
int64 - integer
float64 - float
datetime64 (or timedelta) - datetime
Converting column types
.astype("float")

==========
# Print the head of the hits column
#print(volunteer["hits"].head())

# Convert the hits column to type int
#volunteer["hits"] = volunteer["hits"].astype("int")

# Look at the dtypes of the dataset
#print(volunteer.dtypes)

------------------------------------------------------------------------------------------------------------
Class distribution

How do you split train/test when your samples are not normally distributed?
Stratified sampling
from train_test_split method .value_counts()
parameter for train_test_split is "stratify="

# Create a data with all columns except category_desc
#volunteer_X = volunteer.drop("category_desc", axis=1)

# Create a category_desc labels dataset
#volunteer_y = volunteer[["category_desc"]]

# Use stratified sampling to split up the dataset according to the volunteer_y dataset
#X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)

# Print out the category_desc counts on the training y labels
#print(y_train["category_desc"].value_counts())

------------------------------------------------------------------------------------------------------------
Standardizing Data

scikit-learn models assume normally distributed data
applied to continuous numerical data
linearity assumptions
types discussed
log normalization
feature scaling
when to standardize models
model in linear space
dataset features have high variance
dataset features are continuous and on different scales

============
# Split the dataset and labels into training and test sets
#X_train, X_test, y_train, y_test = train_test_split(X, y)

# Fit the k-nearest neighbors model to the training data
#knn.fit(X_train, y_train)

# Score the model on the test data
#print(knn.score(X_test, y_test))

#################################################
#<script.py> output:
#    0.5333333333333333
#################################################
#You can see that the accuracy score is pretty low. Let's explore
#methods to improve this score.
------------------------------------------------------------------------------------------------------------

Log normalization

applies log transformation to feature(s) with high variance relative to other features
helps feature(s) approach normality
takes the log of e (2.718)
e.g. log 30 = 3.4, log 300 = 5.7, log 3000 = 8
captures relative changes, the magnitude of change, and keeps everything in the positive space

============
#Numpy has been imported as np in your workspace.

# Print out the variance of the Proline column
#print(wine["Proline"].var())

# Apply the log normalization function to the Proline column
#wine["Proline_log"] = np.log(wine["Proline"])

# Check the variance of the normalized Proline column
#print(wine["Proline_log"].var())

#################################################
#<script.py> output:
#    99166.71735542436
#    0.17231366191842012
#################################################
#  The np.log() function is an easy way to log normalize a column.

------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
Scaling data for feature comparison
------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------

What is feature scaling?
features on different scales
model with linear characteristics
center features with mean of zero and transform unit variance to same
transforms to approximately normal distribution

===============
# Import StandardScaler from scikit-learn
#from sklearn.preprocessing import StandardScaler

# Create the scaler
#ss = StandardScaler()

# Take a subset of the DataFrame you want to scale
#wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]

# Apply the scaler to the DataFrame subset
#wine_subset_scaled = ss.fit_transform(wine_subset)

wine[['Ash', 'Alcalinity of ash', 'Magnesium']].describe()

------------------------------------------------------------------------------------------------------------
Scaling data - standardizing columns
Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.

Instructions
Import the StandardScaler class.
Instantiate a StandardScaler() and store it in the variable, scaler.
Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.
Fit and transform the standard scaler to wine_subset.

# Import StandardScaler
from sklearn.preprocessing import StandardScaler

# Create the scaler
scaler = StandardScaler()

# Subset the DataFrame you want to scale 
wine_subset = wine[['Ash','Alcalinity of ash','Magnesium']]

# Apply the scaler to wine_subset
wine_subset_scaled = scaler.fit_transform(wine_subset)

------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
Standardized data and modeling
------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
K-nearest Neighbors (KNN)
The k-nearest neighbors (KNN) algorithm is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of 
an individual data point. It is one of the popular and simplest classification and regression classifiers used in machine learning today.
------------------------------------------------------------------------------------------------------------
WORKFLOW FOR TRAINING A MODEL IN SCIKIT-LEARN
The workflow for training a model in scikit-learn starts with splitting the data into a training and test set. 
This can be done with scikit-learn's train_test_split function. 
Splitting the data will allow us to evaluate the model's performance using unseen data, rather than evaluating its performance on the data it was trained on. 
Once the data has been split, we can begin preprocessing the training data. 
It's really important to split the data prior to preprocessing, so none of the test data is used to train the model. 
When non-training data is used to train the model, this is called data leakage, and it should be avoided so that any performance metrics are reflective of the model's ability to generalize to unseen data. 
We instantiate a k-neighbors classifier and a standard scaler to scale our features. 
Here, we preprocess and fit the training features using the fit_transform method, and preprocess the test features using the transform method. 
Using the transform method means that the test features won't be used to fit the model and avoids data leakage. 
Now that we've finished preprocessing, we can fit the KNN model to the scaled training features, and return the test set accuracy using the score method on the scaled test features and test labels.

------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
KNN on non-scaled data
Before adding standardization to your scikit-learn workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.

The knn model as well as the X and y data and labels sets have been created already.

Instructions
Split the dataset into training and test sets.
Fit the knn model to the training data.
Print out the test set accuracy of your trained knn model.

# Split the dataset and labels into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))
------------------------------------------------------------------------------------------------------------
KNN on scaled data
The accuracy score on the unscaled wine dataset was decent, but let's see what you can achieve by using standardization. 
Once again, the knn model as well as the X and y data and labels set have already been created for you.

Instructions
Create the StandardScaler() method, stored in a variable named scaler.
Scale the training and test features, being careful not to introduce data leakage.
Fit the knn model to the scaled training data.
Evaluate the model's performance by computing the test set accuracy.

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# Instantiate a StandardScaler
scaler = StandardScaler()

# Scale the training and test features
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train_scaled, y_train)

# Score the model on the test data
print(knn.score(X_test_scaled, y_test))
------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
Feature engineering

What is feature engineering?
the creation of new features based on existing features
insight into relationships between features
extract and expand data
dataset-dependent

scenarios:
text data
categorical data
time stamps
averages
------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
Encoding categorical variables

encoding binary values
Pandas
.apply() plus lambda function
.get_dummies() for one-hot encoding of variables with two or more labels
scikit-learn
LabelEncoder
------------------------------------------------------------------------------------------------------------
#Encoding categorical variables - binary

#Take a look at the hiking dataset. There are several columns here
#that need encoding, one of which is the Accessible column, which
#needs to be encoded in order to be modeled. Accessible is a binary
#feature, so it has two values - either Y or N - so it needs to be
#encoded into 1s and 0s. Use scikit-learn's LabelEncoder method to
#do that transformation.

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])

# Compare the two columns
print(hiking[['Accessible', 'Accessible_enc']].head())
------------------------------------------------------------------------------------------------------------
#Encoding categorical variables - one-hot

#One of the columns in the volunteer dataset, category_desc, gives
#category descriptions for the volunteer opportunities listed.
#Because it is a categorical variable with more than two categories,
#we need to use one-hot encoding to transform this column numerically.
#Use Pandas' get_dummies() function to do so.

# Transform the category_desc column
#category_enc = pd.get_dummies(volunteer["category_desc"])

# Take a look at the encoded columns
#print(category_enc.head())
------------------------------------------------------------------------------------------------------------
Engineering numerical features

Aggregation
taking means across columns using .apply(), lambda and .mean()
dates
convert to datetime using .to_datetime() in Pandas
------------------------------------------------------------------------------------------------------------
#Engineering numerical features - taking an average

#A good use case for taking an aggregate statistic to create a new
#feature is to take the mean of columns. Here, you have a DataFrame
#of running times named running_times_5k. For each name in the
#dataset, take the mean of their 5 run times.

# Use .loc to create a mean column
running_times_5k["mean"] = running_times_5k.loc[:, :].mean(axis=1)

# Take a look at the results
print(running_times_5k.head())
------------------------------------------------------------------------------------------------------------
#Engineering numerical features - datetime
#There are several columns in the volunteer dataset comprised of
#datetimes. Let's take a look at the start_date_date column and
#extract just the month to use as a feature for modeling.

# First, convert string column to date column
#volunteer["start_date_converted"] = pd.to_datetime(volunteer["start_date_date"])

# Extract just the month from the converted column
#volunteer["start_date_month"] = volunteer["start_date_converted"].apply(lambda row: row.month)

# Take a look at the converted and new month columns
#print(volunteer[["start_date_converted", "start_date_month"]].head())
------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------
Text classification

Text extraction
  regular expressions
  data = re.match(pattern, data_string)
  data.group(0)
  \d - digits
  as many as possible that are adjacent
  . - decimal point
TF/IDF vectorization
  Term Frequency Inverse Document Frequency
  from sklearn.feature_extraction.text import TfidfVectorizer
  .fit_transform()
Naive Bayes classifier
  P(A|B) = [P(B|A)*P(A)] / P(B)
  treats each feature as independent of the other
------------------------------------------------------------------------------------------------------------
#Engineering features from strings - extraction

#The Length column in the hiking dataset is a column of strings, but
#contained in the column is the mileage for the hike. We're going to
#extract this mileage using regular expressions, and then use a lambda
#in Pandas to apply the extraction to the DataFrame.

# Write a pattern to extract numbers and decimals
def return_mileage(length):
    
    # Search the text for matches
    mile = re.search('\d+\.\d+', length)
    
    # If a value is returned, use group(0) to return the found value
    if mile is not None:
        return float(mile.group(0))
        
# Apply the function to the Length column and take a look at both columns
hiking["Length_num"] = hiking['Length'].apply(return_mileage)
print(hiking[["Length", "Length_num"]].head())
------------------------------------------------------------------------------------------------------------
#Engineering features from strings - tf/idf

#Let's transform the volunteer dataset's title column into a text
#vector, to use in a prediction task in the next exercise.

# Take the title text
#title_text = volunteer["title"]

# Create the vectorizer method
#tfidf_vec = TfidfVectorizer()

# Transform the text into tf-idf vectors
#text_tfidf = tfidf_vec.fit_transform(title_text)










